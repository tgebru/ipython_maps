{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train features...\n",
      "loading training labels and lat/longs...\n"
     ]
    }
   ],
   "source": [
    "#Create a file with xi,xj, cost_source-->xi, cost_xi-->sink\n",
    "#CRF connecting points that are geographically close to each other\n",
    "#Also connecting points \n",
    "#File will be read by mincut/maxflow c++ code to construct graph \n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "#sys.path.insert(0, '/scr/r6/tgebru/tools/liblinear-2.1/python')\n",
    "sys.path.insert(0,'/imagenetdb3/tgebru/software/liblinear-2.1/python')\n",
    "from liblinearutil import *\n",
    "\n",
    "#FEAT='fc7'\n",
    "FEAT='prob'\n",
    "USE_SVM=False\n",
    "train_feat_pname='/imagenetdb3/tgebru/cvpr2016/housing_train_2013_class_%s'%FEAT\n",
    "svm_model_name='/imagenetdb3/tgebru/cvpr2016/housing_val_2013_class_%s.model'%FEAT\n",
    "train_data_pname='/imagenetdb3/tgebru/cvpr2016/housing_train_2013_class_cnn_data.pickle'\n",
    "\n",
    "#Load features for training labels\n",
    "print 'loading train features...'\n",
    "with open(train_feat_pname,'rb') as f:\n",
    "  train_feats = pickle.load(f)\n",
    "\n",
    "#Load SVM weights\n",
    "if USE_SVM:\n",
    "    print 'loading SVM weights...'\n",
    "    model=load_model(svm_model_name)\n",
    "    #Get SVM Weights\n",
    "    #Decision function=W*x_ithlabel+b_ithlabel\n",
    "    [w_0, b_0]=model.get_decfun(label_idx=0)\n",
    "    \n",
    "#Load lat,lng,actual,predicted labels for training\n",
    "print 'loading training labels and lat/longs...'\n",
    "with open(train_data_pname,'rb') as f:\n",
    "    train_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location 0 out of 18159\n",
      "location 1000 out of 18159\n",
      "location 2000 out of 18159\n",
      "location 3000 out of 18159\n",
      "location 4000 out of 18159\n",
      "location 5000 out of 18159\n",
      "location 6000 out of 18159\n",
      "location 7000 out of 18159\n",
      "location 8000 out of 18159\n",
      "location 9000 out of 18159\n",
      "location 10000 out of 18159\n",
      "location 11000 out of 18159\n",
      "location 12000 out of 18159\n",
      "location 13000 out of 18159\n",
      "location 14000 out of 18159\n",
      "location 15000 out of 18159\n",
      "location 16000 out of 18159\n",
      "location 17000 out of 18159\n",
      "location 18000 out of 18159\n",
      "sorting location matarix....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#Feature difference matrix between each point and its K'th neighbors\\nfeat_mat=np.zeros((train_array.shape[0],k))\\nfor i in xrange(feat_array.shape[0]):\\n    if i%1000==0: \\n        print 'features %d out of %d'%(i,feat_array.shape[0])\\n    #L2 distance of features\\n    feat_mat[i,:]=np.linalg.norm(np.subtract(feat_array[i,:],\\n                    feat_array[sorted_loc[i,0:k],:]),axis=1)  \\n\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want all the points in a zipcode to have the same value\n",
    "#Also want nearby zip codes to have similar values?\n",
    "#How do we incorporate multiple images from the same place?\n",
    "\n",
    "#Create adjacency matricies for\n",
    "#Euclidean location distance, feature distance, unary labels\n",
    "\n",
    "#Create numpy matrix of training data\n",
    "\n",
    "train_array=np.asarray(train_data,dtype=np.float)\n",
    "\n",
    "# Euclidean location difference Matrix between all points\n",
    "loc_mat=np.zeros((train_array.shape[0],train_array.shape[0]))\n",
    "for i in xrange(train_array.shape[0]):\n",
    "   if i%1000==0: \n",
    "       print 'location %d out of %d'%(i,train_array.shape[0])\n",
    "   #L2 distance of location\n",
    "   loc_mat[i,:]=np.linalg.norm(np.subtract(train_array[i,0:2],\n",
    "                                           train_array[:,0:2]), axis=1)\n",
    "    \n",
    "#Sort edges for each location in increasing order of euclidean distance\n",
    "print 'sorting location matarix....'\n",
    "sorted_loc=loc_mat.argsort(axis=1)\n",
    "k=60#Neighbors of node to connect edges 6 ims are still in same lat,lng\n",
    "\n",
    "sorted_loc_mat=np.zeros((loc_mat.shape[0],k))\n",
    "for i in xrange(sorted_loc_mat.shape[0]):\n",
    "    sorted_loc_mat[i,:]=loc_mat[i,sorted_loc[i,0:k]]\n",
    "\n",
    "feat_array=np.asarray(train_feats[0:len(train_data)],dtype=np.float)\n",
    "'''\n",
    "#Feature difference matrix between each point and its K'th neighbors\n",
    "feat_mat=np.zeros((train_array.shape[0],k))\n",
    "for i in xrange(feat_array.shape[0]):\n",
    "    if i%1000==0: \n",
    "        print 'features %d out of %d'%(i,feat_array.shape[0])\n",
    "    #L2 distance of features\n",
    "    feat_mat[i,:]=np.linalg.norm(np.subtract(feat_array[i,:],\n",
    "                    feat_array[sorted_loc[i,0:k],:]),axis=1)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for label=0 (lowest housing $$)\n",
    "if USE_SVM:\n",
    "    labels_0=np.copy(train_array[:,2]) #actual label\n",
    "    labels_1=np.copy(train_array[:,2]) #actual label\n",
    "\n",
    "    labels_0[np.where(train_array[:,2]==0)[0]]=1\n",
    "    labels_1[np.where(train_array[:,2]!=0)[0]]=-1\n",
    "\n",
    "    #Unary energies -ln(phi1)=Y_i*W*X_i\n",
    "    unary_energy_0=np.multiply(svm_labels_0,\n",
    "                      np.dot(np.asarray(w_0),np.transpose(feat_array)))\n",
    "\n",
    "    unary_energy_1=np.multiply(svm_labels_1,\n",
    "                      np.dot(np.asarray(w_0),np.transpose(feat_array)))\n",
    "    \n",
    "else:\n",
    "    #Make lables into binary classes 0,1 and 2,3\n",
    "    feat_array=np.asarray(train_feats[0:len(train_data)],dtype=np.float)\n",
    "    probs_1=feat_array[:,0]+feat_array[:,1] #-->classes 0&1 are labeled 1\n",
    "    probs_0=1-probs_1                       #-->classes 2,3 are labeled 0\n",
    "    unary_energy_0= -np.log(probs_1)\n",
    "    unary_energy_1= -np.log(probs_0)\n",
    "    \n",
    "\n",
    "    \n",
    "actual_labels=train_array[:,2]\n",
    "actual_bin_labels=(actual_labels==0)+(actual_labels==1)\n",
    "cnn_bin_labels=np.argmax(np.array([probs_0, probs_1]),axis=0)\n",
    "#Create binary energies\n",
    "USE_VICENTE=False\n",
    "max_acc=0\n",
    "res_list=[]\n",
    "\n",
    "if USE_VICENTE:\n",
    "    #Alpha1 and Alpha2 from Vincente's paper\n",
    "    alpha1=1e-3\n",
    "    alpha2=1e-3\n",
    "    binary_energy=np.add(np.divide(alpha1,sorted_loc_mat),\n",
    "                           np.divide(alpha2,feat_mat))\n",
    "\n",
    "    binary_energy[np.where(np.isinf(binary_energy))]=np.finfo('float').max\n",
    "\n",
    "else:\n",
    "    w1_list=[0,0.5,1,5,10,50,100,1000]\n",
    "    w2_list=[0,0.5,1,5,10,50,100,1000] \n",
    "    ebs_list=[0,0.1,0.5,1,5,10]\n",
    "    for w1 in w1_list:\n",
    "        for w2 in w2_list:\n",
    "            for ebs in ebs_list:\n",
    "                print 'trying w1=%d and w2=%d and ebs=%d'%(w1,w2,ebs)\n",
    "                binary_energy=w1*(ebs+np.exp(-w2*sorted_loc_mat))/(ebs+1)\n",
    "\n",
    "                #Write these out to file for c program to read\n",
    "\n",
    "                #Write (xi,xj,cost_ij)\n",
    "                binary_energy_file='/imagenetdb3/tgebru/cvpr2016/binary_energies.txt'\n",
    "                print 'Writing binary energies to file...'\n",
    "                with open(binary_energy_file, 'wb') as f:\n",
    "                    for i in xrange(binary_energy.shape[0]):\n",
    "                        for j in xrange(k):\n",
    "                            f.write('%d,%d,%f\\n'%(i,j,binary_energy[i,j]))\n",
    "\n",
    "                #Write (xi, cost_source_xi, cost_xi_sink)\n",
    "                unary_energy_file='/imagenetdb3/tgebru/cvpr2016/unary_energies.txt'\n",
    "                print 'Writing unary energies to file...'\n",
    "                with open(unary_energy_file,'wb')as f:\n",
    "                    for i in xrange(unary_energy_0.shape[0]):\n",
    "                        f.write('%d,%f,%f\\n'%(i,unary_energy_1[i],unary_energy_0[i]))\n",
    "\n",
    "                #Run maxflow/mincut for different parameters\n",
    "                import os\n",
    "                flow_res_file='/imagenetdb3/tgebru/cvpr2016/flow_results.txt'\n",
    "                code_dir='/afs/cs.stanford.edu/u/tgebru/cvpr2016/code/maxflow/'\n",
    "                os.system('g++ -o %s/tg_out %s/graph.cpp %s/maxflow.cpp %s/tg_get_mincut.cpp'%(code_dir,code_dir,code_dir,code_dir))\n",
    "                os.system('%s'%os.path.join(code_dir,'./tg_out'))\n",
    "\n",
    "                node_labels=open(flow_res_file,'rb').readlines()\n",
    "                predicted_labels=np.asarray([n.split(',')[-1].strip() \n",
    "                                             for n in node_labels],dtype='int')\n",
    "\n",
    "                with open('/imagenetdb3/tgebru/cvpr2016/housing_train_2013_class_cnn.pickle', 'rb') as f:\n",
    "                    cnn_labels=pickle.load(f)\n",
    "\n",
    "                cnn_labels=np.asarray(cnn_labels)\n",
    "                #print np.where(actual_labels==cnn_labels)[0].shape[0]/float(cnn_labels.shape[0])\n",
    "                #print np.where(actual_labels==predicted_labels)[0].shape[0]/float(cnn_labels.shape[0])\n",
    "                print np.where(cnn_bin_labels==predicted_labels)[0].shape[0]/float(cnn_bin_labels.shape[0])\n",
    "                print np.where(actual_bin_labels==cnn_bin_labels)[0].shape[0]/float(cnn_bin_labels.shape[0])\n",
    "                acc= np.where(actual_bin_labels==predicted_labels)[0].shape[0]/float(actual_bin_labels.shape[0])\n",
    "                if acc>max_acc:\n",
    "                    max_acc=acc\n",
    "                    max_w1=w1\n",
    "                    max_w2=w2\n",
    "                    max_ebs=ebs\n",
    "                res_list.append((w1,w2,ebs,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
